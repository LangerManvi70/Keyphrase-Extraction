{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJXguP2e9R7X"
   },
   "source": [
    "Keyphrase Extraction using Word2Vec with Hierarchical Clustering\n",
    "\n",
    "1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWPs4XGp56qh"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dcc42418156a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk,re, pprint\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk.regexp import tag_pattern2re_pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSeWEosB9IPu"
   },
   "source": [
    "2. Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VX8tQ2Nm59a_"
   },
   "outputs": [],
   "source": [
    "# Parameters Set\n",
    "n_clusters = 100    # No. Of Clusters\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Actual_Summary = \"output.txt\"\n",
    "\n",
    "\n",
    "# Text Processing : Handling StopWords, Tokenization\n",
    "def text_processing(file):\n",
    "    f = open(file, 'r', encoding='utf-8')\n",
    "    text = f.read()\n",
    "    \n",
    "    #Tokenization : Convert to sentences and then to words\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        words = [w.lower() for w in words if len(w)>2 and w not in stop_words]\n",
    "        words = [w.lower() for w in words if w != \"the\"]\n",
    "        data.append(words)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "text = text_processing(\"input.txt\")\n",
    "print(\"Text Processing : done\")\n",
    "\n",
    "\n",
    "print(text)\n",
    "\n",
    "count_frequency = {}\n",
    "for line in text:\n",
    "    for w in line:\n",
    "        count_frequency[w] = 0\n",
    "        \n",
    "for line in text:\n",
    "    for w in line:\n",
    "        count_frequency[w] = count_frequency[w] + 1\n",
    "            \n",
    "candidate_term = len(count_frequency.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emMb9Zof9bAA"
   },
   "source": [
    "3. Implement Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vN0MSAd06Da8"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(text, size=300, window = 10, min_count=1)\n",
    "words = list(model.wv.vocab)\n",
    "\n",
    "print(\"Word2Vec Model : done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EC4bB-846gIE"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_closestwords_tsnescatterplot(model, word):\n",
    "    \n",
    "    arr = np.empty((0,300), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = model.similar_by_word(word)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()\n",
    "    \n",
    "display_closestwords_tsnescatterplot(model, words[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfaYaT2p9iR3"
   },
   "source": [
    "4. Hierarchical Clustering Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSib28oS6gXM"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "l = linkage(model.wv.syn0, method='complete', metric='seuclidean')\n",
    "\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(250, 100))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.ylabel('word')\n",
    "plt.xlabel('distance')\n",
    "\n",
    "dendrogram(\n",
    "    l,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=16.,  # font size for the x axis labels\n",
    "    orientation='left',\n",
    "    leaf_label_func=lambda v: str(model.wv.index2word[v])\n",
    ")\n",
    "# plt.show()\n",
    "print(\"Hierarchical Clustering : done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72WU4FVY9m4l"
   },
   "source": [
    "5. Applied K-Means and Regular Expression Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RF8Wqvtw6gkG"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "n_clusters = int(candidate_term*4/5)\n",
    "\n",
    "print(\"\\n clusters : \", n_clusters)\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc = AgglomerativeClustering(n_clusters, affinity=\"euclidean\", linkage='complete')\n",
    "y_hc = hc.fit_predict(model.wv.syn0)\n",
    "\n",
    "print(\"***************************************************************************************************\")\n",
    "\n",
    "# Extracting The Keyword by choosing the mean of cluster\n",
    "summary=[]\n",
    "def Extracted_Keywords(clus, clus_name):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=1, random_state=0).fit(clus)\n",
    "    labels = kmeans.labels_\n",
    "    mean = kmeans.cluster_centers_.reshape(300,1)\n",
    "    min_to_centre = []\n",
    "    \n",
    "    for i in clus:\n",
    "        Y = distance.cdist(i.reshape(300,1), mean, 'euclidean')\n",
    "        sum_Y = np.sum(Y)\n",
    "        min_to_centre.append(sum_Y)\n",
    "\n",
    "    min_value = min(min_to_centre)\n",
    "    ind_value = np.argmin(min_to_centre)\n",
    "    \n",
    "#     print(ind_value)\n",
    "#     print(clus_name[ind_value])\n",
    "    summary.append(clus_name[ind_value])\n",
    "\n",
    "for j in range(0,n_clusters):\n",
    "    clus = []\n",
    "    clus_name = []\n",
    "    count = 0\n",
    "    for i in range(len(y_hc)):\n",
    "        if(y_hc[i] == j):\n",
    "            count += 1\n",
    "            clus.append(model[words[i]])\n",
    "            clus_name.append(words[i])\n",
    "    \n",
    "    Extracted_Keywords(clus, clus_name)\n",
    "\n",
    "\n",
    "    \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZvSOZFlR6rW5"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "summary_frequency = {}\n",
    "\n",
    "for word in summary:\n",
    "    summary_frequency[word] = count_frequency[word]\n",
    "    \n",
    "od = sorted(summary_frequency.items(), key=lambda x: x[1])\n",
    "od = od[::-1]\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(0,20):\n",
    "    result.append(od[i][0])\n",
    "\n",
    "print(od)\n",
    "\n",
    "    \n",
    "    \n",
    "# Regular Expression Parser\n",
    "grammar = \"chunk:{(<JJ>)*(<NN | NNS | NNP>)+} \" \n",
    "chunkParser= RegexpParser(grammar)\n",
    "tagged = nltk.pos_tag(result)\n",
    "result = chunkParser.parse(tagged)\n",
    "for subtree in result.subtrees():\n",
    "    print(subtree)\n",
    "# result.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQv_EDfk6rn5"
   },
   "outputs": [],
   "source": [
    "right_lis=[]\n",
    "def getNodes(parent):\n",
    "    print(parent.label())\n",
    "    for node in parent:\n",
    "      \n",
    "        if type(node) is nltk.Tree:\n",
    "            if node.label() == 'chunk':\n",
    "                print(\"hii\")\n",
    "                #print(node.leaves())\n",
    "                right_lis.append(node.leaves())\n",
    "                #print(\"Sentence:\", \" \".join(node.leaves()))  \n",
    "                \n",
    "            \n",
    "            else:\n",
    "                print(node.label())\n",
    "        else:\n",
    "            pass\n",
    "        print(node)  \n",
    "        \n",
    "getNodes(result)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXrF0AE76gvi"
   },
   "outputs": [],
   "source": [
    "\n",
    "keyphrase=[]\n",
    "str1=''\n",
    "for i in range(len(right_lis)):\n",
    "    for j in range(len(right_lis[i])):\n",
    "        for k in range(len(right_lis[i][j])-1):\n",
    "            if right_lis[i][j][k+1] != 'JJ':\n",
    "             \n",
    "                str1=str1+right_lis[i][j][k]\n",
    "                keyphrase.append(str1)\n",
    "                str1=''\n",
    "            else:\n",
    "                str1=str1+right_lis[i][j][k] + ' '\n",
    "    str1=' '\n",
    "print(right_lis)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Keyphrases : \",keyphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIOJqvP79uYP"
   },
   "source": [
    "6. Results : Precision, Recall, F-Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-B74RxO-64i9"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "def Calculate_Score():\n",
    "    str2 = \"\"\n",
    "    for i in summary:\n",
    "        str2 = str2 + i\n",
    "        str2 = str2 + \" \"\n",
    "    \n",
    "    print(\"Predicted Summary : \\n\")\n",
    "    print(str2)\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    f = open(Actual_Summary, 'r', encoding='utf-8')\n",
    "    text = f.read()\n",
    "    \n",
    "    print(\"Actual Summary : \\n\")\n",
    "    print(text)\n",
    "\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(text, str2)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    print(\"F-Measure : \", scores[0]['rouge-1']['f'])\n",
    "    print(\"Precision : \", scores[0]['rouge-1']['p'])\n",
    "    print(\"Recall    : \", scores[0]['rouge-1']['r'])\n",
    "    \n",
    "Calculate_Score()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
